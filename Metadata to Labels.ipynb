{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_root = 'data/drums/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from os.path import join\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import sklearn\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time metadata = json.load(open(join(data_root, 'metadata.json')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(metadata)\n",
    "metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# geotags = [m['geotag'] for m in metadata if m['geotag'] is not None]\n",
    "usernames = [m['username'] for m in metadata]\n",
    "names = [m['name'] for m in metadata]\n",
    "tags = [m['tags'] for m in metadata]\n",
    "descriptions = [m['description'] for m in metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = Counter(usernames)\n",
    "print len(usernames), len(set(usernames))\n",
    "plt.plot([count for username, count in c.most_common()[:1000]])\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fingerprints = [' '.join([m['name'], m['description']] + m['tags']) for m in metadata]\n",
    "fingerprints = [f.replace('_', ' ') for f in fingerprints] # split on underscore (vectorizer doesn't by default)\n",
    "fingerprints = [f.replace('-', '_') for f in fingerprints] # don't split on dash (vectorizer does by default)\n",
    "fingerprints = [re.sub('wav|aif|mp3', '', f) for f in fingerprints] # remove wav, aif, mp3\n",
    "fingerprints = [re.sub('<a.+?/a>', '', f) for f in fingerprints]\n",
    "# altnernatively, this could be done with a whitelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "username_dict = collections.defaultdict(list)\n",
    "for username, fingerprint in zip(usernames, fingerprints):\n",
    "    username_dict[username].append(fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in [10,11,12]:\n",
    "    print username_dict[usernames[3000]][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_unique = []\n",
    "ratios = []\n",
    "ratio_cutoff = 0.99\n",
    "matcher = SequenceMatcher(isjunk=lambda x: x in '. \\t\\n', autojunk=False)\n",
    "for username in tqdm(username_dict, leave=True):\n",
    "    cur = username_dict[username]\n",
    "    unique = []\n",
    "    for a in cur:\n",
    "        matcher.set_seq1(a)\n",
    "        max_ratio = 0\n",
    "        for b in unique:\n",
    "            matcher.set_seq2(b)\n",
    "            ratio = matcher.real_quick_ratio()\n",
    "            ratios.append(ratio)\n",
    "            max_ratio = max(max_ratio, ratio)\n",
    "        if max_ratio < ratio_cutoff:\n",
    "            unique.append(a)\n",
    "    all_unique.extend(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(all_unique),'out of',len(set(descriptions)),'unique'\n",
    "print 'did ratio', len(ratios), 'ratio comparisons'\n",
    "ratios.sort()\n",
    "plt.plot(ratios)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print all_unique[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# could be good to do per-user vectorization with a low max_df cutoff\n",
    "# then combine the vectorized results\n",
    "vectorizer = CountVectorizer(min_df=2, stop_words='english', binary=True)\n",
    "vectors = vectorizer.fit_transform(fingerprints)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs = [(word, vectors.getcol(idx).sum()) for word, idx in vectorizer.vocabulary_.items()]\n",
    "#sort from largest to smallest\n",
    "print sorted (freqs, key = lambda x: -x[1])[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pair in sorted (freqs, key = lambda x: -x[1])[:1000]:\n",
    "    print pair[1], pair[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "synsets = [l.strip().replace(' ','').split(',') for l in open(data_root + '/synsets.txt').readlines()]\n",
    "synset_examples = [set() for s in synsets] # make one set per synset\n",
    "leftovers = Counter()\n",
    "for doc_index, doc in enumerate(tqdm(vectorizer.inverse_transform(vectors), leave=True)):\n",
    "    matches = 0\n",
    "    for term in doc:\n",
    "        for synset_example, synset in zip(synset_examples, synsets):\n",
    "            for syn in synset:\n",
    "                if term == syn:\n",
    "#                     print doc_index, synset_index, term\n",
    "                    synset_example.add(doc_index)\n",
    "                    matches += 1\n",
    "                    break\n",
    "    if matches == 0:\n",
    "        leftovers.update(doc)\n",
    "leftovers.most_common()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(synset_examples)\n",
    "for synset_example, synset in zip(synset_examples, synsets):\n",
    "    print synset, len(synset_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(data_root + 'synset_examples.pkl', 'wb') as f:\n",
    "    pickle.dump(synset_examples, f, -1)\n",
    "with open(data_root + 'synsets.pkl', 'wb') as f:\n",
    "    pickle.dump(synsets, f, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 1000\n",
    "print vectorizer.get_feature_names()[i:i+100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# average number of tokens per document\n",
    "vectors.nnz / float(vectors.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=100, max_iter=10)\n",
    "%time fit = lda.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "            for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_top_words(lda, vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = 0\n",
    "b = 1\n",
    "c = 2\n",
    "ref = all_unique\n",
    "n = range(len(fit[0]))\n",
    "\n",
    "for i in [6,7,8,9,10]:\n",
    "    print ref[i]\n",
    "    plt.bar(n, fit[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vector = vectorizer.transform(['laser'])\n",
    "print vector\n",
    "plt.bar(n, lda.transform(vector)[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lesson is: many sounds aren't going to have very good descriptions, and the fingerprint isn't going to be enough to distinguish them from each other. We definitely can't run t-SNE on the LDA results, maybe on the (huge) sparse matrix...\n",
    "\n",
    "The right thing to do as a next step is to use the tags to do multiclass classification on some categories we decide on in advance, do data augmentation, etc. Treat it as a typical supervised learning problem, then run t-SNE on the \"soft\", pre-output vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "searchable = [m['name'] + ' ' + ' '.join(m['tags']) for m in metadata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(data_root + 'searchable.txt', 'w') as f:\n",
    "    for line in searchable:\n",
    "        print>>f, line.encode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
